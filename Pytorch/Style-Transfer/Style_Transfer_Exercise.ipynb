{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Style Transfer  using convolutional neural networks\n",
    "\n",
    "\n",
    "It is an interpretation of the paper [Image Style Transfer Using Convolutional Neural Networks, by Gatys](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf) using PyTorch.\n",
    "\n",
    "To do the style transfer the VGG-19 network is used with its pretrained features. It consists of 5 convolutional neural layers and 3 fully connected layers. For the process, the style of an image and the content of another are separated, and a new third image is optimized with the style and content of the previously selected. With some iteratios the target image will have the selected result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import resources\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the \"features\" portion of VGG19 (I will not need the \"classifier\" portion)\n",
    "vgg = models.vgg19(pretrained=True).features\n",
    "\n",
    "# freeze all VGG parameters since I'am only optimizing the target image\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the model to GPU, if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vgg.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Content and Style Images\n",
    "\n",
    "First a pair of helper functions are added for loading the image as normalized tensors and then another one to convert it again to a numpy image. It is easier to work with tensors and smaller images for deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, max_size=400, shape=None):\n",
    "    ''' Load in and transform an image, making sure the image\n",
    "       is <= 400 pixels in the x-y dims.'''\n",
    "    \n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    # large images will slow down processing\n",
    "    if max(image.size) > max_size:\n",
    "        size = max_size\n",
    "    else:\n",
    "        size = max(image.size)\n",
    "    \n",
    "    if shape is not None:\n",
    "        size = shape\n",
    "        \n",
    "    in_transform = transforms.Compose([\n",
    "                        transforms.Resize(size),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
    "    image = in_transform(image)[:3,:,:].unsqueeze(0)    \n",
    "    return image\n",
    "\n",
    "# helper function for un-normalizing an image \n",
    "# and converting it from a Tensor image to a NumPy image for display\n",
    "def im_convert(tensor):\n",
    "    \"\"\" Display a tensor as an image. \"\"\"\n",
    "    \n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then load the content and style images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in content and style image\n",
    "content = load_image('images/space_needle.jpg').to(device)\n",
    "# Resize style to match content, makes code easier\n",
    "style = load_image('images/StarryNightVanGogh.jpg', shape=content.shape[-2:]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## VGG19 Layers\n",
    "\n",
    "The images are passed through the VGG-19 network until the desired output is obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out VGG19 structure to see the names of various layers\n",
    "print(vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content and Style Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(image, model, layers=None):\n",
    "    \"\"\" Run an image forward through a model and get the features for \n",
    "        a set of layers. Default layers are for VGGNet matching Gatys et al (2016)\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Mapping layer names of PyTorch's VGGNet\n",
    "    ## Need the layers for the content and style representations of an image\n",
    "    if layers is None:\n",
    "        layers = {'0': 'conv1_1',\n",
    "                 '5': 'conv2_1',\n",
    "                 '10': 'conv3_1',\n",
    "                 '19': 'conv4_1',\n",
    "                 '21': 'conv4_2',\n",
    "                 '28': 'conv5_1'}\n",
    "        \n",
    "    features = {}\n",
    "    x = image\n",
    "    # model._modules is a dictionary holding each module in the model\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Gram Matrix \n",
    "\n",
    "The output of every convolutional layer is a Tensor with dimensions associated with the `batch_size`, a depth, `d` and some height and width (`h`, `w`). The Gram matrix of a convolutional layer can be calculated as follows:\n",
    "* Get the depth, height, and width of a tensor using `batch_size, d, h, w = tensor.size`\n",
    "* Reshape that tensor so that the spatial dimensions are flattened\n",
    "* Calculate the gram matrix by multiplying the reshaped tensor by it's transpose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    \"\"\" Calculate the Gram Matrix of a given tensor \n",
    "        Gram Matrix: https://en.wikipedia.org/wiki/Gramian_matrix\n",
    "    \"\"\"\n",
    "\n",
    "    ## get the batch_size, depth, height, and width of the Tensor\n",
    "    _, d, h, w = tensor.size()\n",
    "    \n",
    "    ## reshape it, so I'm multiplying the features for each channel\n",
    "    tensor = tensor.view(d, h * w)\n",
    "    \n",
    "    ## calculate the gram matrix\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    \n",
    "    return gram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Putting it all Together\n",
    "\n",
    "Now that I've written functions for extracting features and computing the gram matrix of a given convolutional layer; let's put all these pieces together! I'll extract features from the images and calculate the gram matrices for each layer in my style representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get content and style features only once before forming the target image\n",
    "content_features = get_features(content, vgg)\n",
    "style_features = get_features(style, vgg)\n",
    "\n",
    "# calculate the gram matrices for each layer of the style representation\n",
    "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "\n",
    "# create a third \"target\" image and prep it for change\n",
    "# it is a good idea to start of with the target as a copy of the *content* image\n",
    "# then iteratively change its style\n",
    "target = content.clone().requires_grad_(True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loss and Weights\n",
    "\n",
    "#### Individual Layer Style Weights\n",
    "\n",
    "Below, I weight the style representation at each relevant layer. It's suggested to use a range between 0-1 to weight these layers. By weighting earlier layers (`conv1_1` and `conv2_1`) more, I expect to get _larger_ style artifacts in my resulting, target image. Should I choose to weight later layers, I'll get more emphasis on smaller features. This is because each layer is a different size and together they create a multi-scale style representation!\n",
    "\n",
    "#### Content and Style Weight\n",
    "\n",
    "Just like in the paper, I define an alpha (`content_weight`) and a beta (`style_weight`). This ratio will affect how _stylized_ my final image is. It's recommended to leave the content_weight = 1 and set the style_weight to achieve the desired ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights for each style layer \n",
    "# weighting earlier layers more will result in *larger* style artifacts\n",
    "# notice I am excluding `conv4_2` from content representation\n",
    "style_weights = {'conv1_1': 1.,\n",
    "                 'conv2_1': 0.75,\n",
    "                 'conv3_1': 0.2,\n",
    "                 'conv4_1': 0.2,\n",
    "                 'conv5_1': 0.2}\n",
    "\n",
    "content_weight = 1  # alpha\n",
    "style_weight = 1e9  # beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the Target & Calculating Losses\n",
    "\n",
    "Now, I'll decide on a number of steps for which to update the image (I am changing the _target_ image and nothing else about VGG19 or any other image). I recommend using at least 2000 steps for good results.\n",
    "\n",
    "Inside the iteration loop, I calculate the content and style losses and update the target image, accordingly.\n",
    "\n",
    "The **content loss** will be the mean squared difference between the target and content features at layer `conv4_2`. This can be calculated as follows: \n",
    "```\n",
    "content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
    "```\n",
    "\n",
    "The **style loss** is calculated in a similar way, only I have to iterate through a number of layers, specified by name in the dictionary `style_weights`. \n",
    "> I'll calculate the gram matrix for the target image, `target_gram` and style image `style_gram` at each of these layers and compare those gram matrices, calculating the `layer_style_loss`.\n",
    "\n",
    "Finally, I'll create the **total loss** by adding up the style and content losses and weighting them with the specified alpha and beta!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for displaying the target image, intermittently\n",
    "show_every = 400\n",
    "\n",
    "# iteration hyperparameters\n",
    "optimizer = optim.Adam([target], lr=0.003)\n",
    "steps = 2000  # decide how many iterations to update the image (5000)\n",
    "\n",
    "for ii in range(1, steps+1):\n",
    "    \n",
    "    # get the features from the target image    \n",
    "    target_features = get_features(target, vgg)\n",
    "    \n",
    "    # then calculate the content loss\n",
    "    content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
    "    \n",
    "    # the style loss\n",
    "    # initialize the style loss to 0\n",
    "    style_loss = 0\n",
    "    # iterate through each style layer and add to the style loss\n",
    "    for layer in style_weights:\n",
    "        # get the \"target\" style representation for the layer\n",
    "        target_feature = target_features[layer]\n",
    "        _, d, h, w = target_feature.shape\n",
    "        \n",
    "        # get the target gram matrix\n",
    "        target_gram = gram_matrix(target_feature)\n",
    "        \n",
    "        # get the \"style\" style representation\n",
    "        style_gram = style_grams[layer]\n",
    "        \n",
    "        # the style loss for one layer, weighted appropriately\n",
    "        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "        \n",
    "        # add to the style loss\n",
    "        style_loss += layer_style_loss / (d * h * w)\n",
    "        \n",
    "        \n",
    "    # the *total* loss\n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "    \n",
    "    # update the target image\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # display intermediate images and print the loss\n",
    "    if  ii % show_every == 0:\n",
    "        print('Total loss: ', total_loss.item())\n",
    "        plt.imshow(im_convert(target))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Target Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display content and final, target image\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.imshow(im_convert(content))\n",
    "ax2.imshow(im_convert(target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
