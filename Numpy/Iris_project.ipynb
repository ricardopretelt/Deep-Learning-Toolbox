{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris project\n",
    "\n",
    "[The Iris Dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) can be found in the scikit-learn library. See more information in the official documentation. It consists of 3 different types of irises (targer variables to classify): Setosa, Versicolour and Virginica, but for simplicity just the data for the first 2 are used. The characteristics (Inputs of the model) are: Sepal Length, Sepal Width, Petal Length and Petal Width.\n",
    "The model is created just using base python and numpy. sklearn is used to download the dataset and to divide it in training and test data. The goal is to create a deep learning model of a neural network using the most basic tools of python without any platform as tensorflow or pytorch to understand deeply all the parts in the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data \n",
    "data.shape\n",
    "data_target = iris.target\n",
    "# To simplify the problem, we just use 2 types of irises\n",
    "data=data[:100]\n",
    "data_target=data_target[:100]\n",
    "# Separating the data in training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data_target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Encapsulate our neural network in a class\n",
    "class IrisNetwork:\n",
    "    def __init__(self, X_train, y_train, hidden_nodes = 10, learning_rate = 0.1):\n",
    "\n",
    "        # Assign a seed to our random number generator to ensure we get\n",
    "        # reproducable results during development \n",
    "        np.random.seed(1)\n",
    "        \n",
    "        # Build the network to have the number of hidden nodes and the learning rate that\n",
    "        # were passed into this initializer. Make the same number of input nodes as\n",
    "        # there are input data points and create a single output node.\n",
    "        self.init_network(X_train.shape[1], hidden_nodes, 1, learning_rate)         \n",
    "        \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Store the number of nodes in input, hidden, and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Store the learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights\n",
    "        \n",
    "        # Initialize self.weights_0_1 as a matrix of zeros. These are the weights between\n",
    "        # the input layer and the hidden layer.\n",
    "        self.weights_0_1 = np.zeros((X_train.shape[1], self.hidden_nodes))\n",
    "        \n",
    "        # Initialize self.weights_1_2 as a matrix of random values. \n",
    "        # These are the weights between the hidden layer and the output layer.\n",
    "        self.weights_1_2 = np.random.normal(0.0, self.hidden_nodes**-0.5, (self.hidden_nodes,self.output_nodes))\n",
    "        \n",
    "        # Create the input layer, a two-dimensional matrix with shape \n",
    "        # 1 x input_nodes, with all values initialized to zero\n",
    "        self.layer_0 = np.zeros((1, self.input_nodes))          \n",
    "                      \n",
    "    def sigmoid(self,x):\n",
    "        # Return the result of calculating the sigmoid activation function\n",
    "        # shown in the lectures\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        # Return the derivative of the sigmoid activation function, \n",
    "        # where \"output\" is the original output from the sigmoid fucntion \n",
    "        return output*(1-output)\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        \n",
    "        # make sure out we have a matching number of reviews and labels\n",
    "        assert(len(X_train) == len(y_train))\n",
    "               \n",
    "        # Remember when we started for printing time statistics\n",
    "        start = time.time()\n",
    "        \n",
    "        # Keep track of correct predictions to display accuracy during training \n",
    "        correct_so_far = 0\n",
    "        # loop through all the given reviews and run a forward and backward pass,\n",
    "        # updating weights for every item\n",
    "        for i in range(len(X_train)):                       \n",
    "\n",
    "            # Get the next instance and correct measure\n",
    "            review=X_train[i]\n",
    "            label=y_train[i]\n",
    "            self.layer_0=review.reshape((1,len(review)))\n",
    "            # Implement the forward pass through the network. \n",
    "            # That means use the given review to update the input layer, \n",
    "            # then calculate values for the hidden layer,\n",
    "            # and finally calculate the output layer.\n",
    "            # \n",
    "            # Do not use an activation function for the hidden layer,\n",
    "            # but use the sigmoid activation function for the output layer.\n",
    "            layer_1=np.dot(self.layer_0, self.weights_0_1)\n",
    "            layer_2=np.dot(layer_1, self.weights_1_2)\n",
    "            output_2=self.sigmoid(layer_2)\n",
    "            # Implement the back propagation pass here. \n",
    "            # That means calculate the error for the forward pass's prediction\n",
    "            # and update the weights in the network according to their\n",
    "            # contributions toward the error, as calculated via the\n",
    "            # gradient descent and back propagation algorithms you \n",
    "            # learned in class.\n",
    "            error=label-output_2\n",
    "            output_error_term=self.sigmoid_output_2_derivative(output_2)*error\n",
    "            hidden_error=output_error_term*(self.weights_1_2).T\n",
    "            delta_weights_0_1=np.dot((self.layer_0).T, hidden_error)\n",
    "            delta_weights_1_2=np.dot((layer_1).T,error)\n",
    "            self.weights_1_2 += delta_weights_1_2*self.learning_rate\n",
    "            self.weights_0_1 += delta_weights_0_1*self.learning_rate \n",
    "            # Keep track of correct predictions. To determine if the prediction was\n",
    "            # correct, check that the absolute value of the output error \n",
    "            # is less than 0.5. If so, add one to the correct_so_far count.\n",
    "            if abs(error)<0.5:\n",
    "                correct_so_far+=1\n",
    "            # For debug purposes, print out our prediction accuracy and speed \n",
    "            # throughout the training process. \n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(X_train)))[:4] \\\n",
    "                             + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) \\\n",
    "                             + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4])\n",
    "            if(i % 25 == 0):\n",
    "                print(\"\")\n",
    "            \n",
    "    def run(self, review):\n",
    "        # TODO: Run a forward pass through the network, like you did in the\n",
    "        #       \"train\" function. That means use the given review to \n",
    "        #       update the input layer, then calculate values for the hidden layer,\n",
    "        #       and finally calculate the output layer.\n",
    "        self.layer_0=review.reshape((1,len(review)))\n",
    "        layer_1=np.dot(self.layer_0, self.weights_0_1)\n",
    "        layer_2=np.dot(layer_1, self.weights_1_2)\n",
    "        output_2=self.sigmoid(layer_2)\n",
    "        # The output layer should now contain a prediction. \n",
    "        # Return 1 for predictions greater-than-or-equal-to `0.5`, \n",
    "        # and 0 otherwise.\n",
    "        \n",
    "        if output_2>= 0.5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0        \n",
    "        \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \"\"\"\n",
    "        Attempts to predict the labels for the given testing_reviews,\n",
    "        and uses the test_labels to calculate the accuracy of those predictions.\n",
    "        \"\"\"\n",
    "        \n",
    "        # keep track of how many correct predictions we make\n",
    "        correct = 0\n",
    "\n",
    "        # we'll time how many predictions per second we make\n",
    "        start = time.time()\n",
    "\n",
    "        # Loop through each of the given reviews and call run to predict\n",
    "        # its label. \n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "           \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \" #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) \\\n",
    "                             + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the model\n",
    "mlp = IrisNetwork(X_train, y_train, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress:0.0 #Correct:1 #Tested:1 Testing Accuracy:100.%\r",
      "Progress:5.0 #Correct:2 #Tested:2 Testing Accuracy:100.%\r",
      "Progress:10.0 #Correct:3 #Tested:3 Testing Accuracy:100.%\r",
      "Progress:15.0 #Correct:3 #Tested:4 Testing Accuracy:75.0%\r",
      "Progress:20.0 #Correct:3 #Tested:5 Testing Accuracy:60.0%\r",
      "Progress:25.0 #Correct:3 #Tested:6 Testing Accuracy:50.0%\r",
      "Progress:30.0 #Correct:3 #Tested:7 Testing Accuracy:42.8%\r",
      "Progress:35.0 #Correct:4 #Tested:8 Testing Accuracy:50.0%\r",
      "Progress:40.0 #Correct:4 #Tested:9 Testing Accuracy:44.4%\r",
      "Progress:45.0 #Correct:4 #Tested:10 Testing Accuracy:40.0%\r",
      "Progress:50.0 #Correct:4 #Tested:11 Testing Accuracy:36.3%\r",
      "Progress:55.0 #Correct:4 #Tested:12 Testing Accuracy:33.3%\r",
      "Progress:60.0 #Correct:5 #Tested:13 Testing Accuracy:38.4%\r",
      "Progress:65.0 #Correct:5 #Tested:14 Testing Accuracy:35.7%\r",
      "Progress:70.0 #Correct:6 #Tested:15 Testing Accuracy:40.0%\r",
      "Progress:75.0 #Correct:6 #Tested:16 Testing Accuracy:37.5%\r",
      "Progress:80.0 #Correct:7 #Tested:17 Testing Accuracy:41.1%\r",
      "Progress:85.0 #Correct:8 #Tested:18 Testing Accuracy:44.4%\r",
      "Progress:90.0 #Correct:8 #Tested:19 Testing Accuracy:42.1%\r",
      "Progress:95.0 #Correct:8 #Tested:20 Testing Accuracy:40.0%"
     ]
    }
   ],
   "source": [
    "# Test without training\n",
    "mlp.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress:0.0 #Correct:0 #Trained:1 Training Accuracy:0.0\n",
      "\r",
      "Progress:1.25 #Correct:1 #Trained:2 Training Accuracy:50.0\r",
      "Progress:2.5 #Correct:1 #Trained:3 Training Accuracy:33.3\r",
      "Progress:3.75 #Correct:1 #Trained:4 Training Accuracy:25.0\r",
      "Progress:5.0 #Correct:1 #Trained:5 Training Accuracy:20.0\r",
      "Progress:6.25 #Correct:1 #Trained:6 Training Accuracy:16.6\r",
      "Progress:7.5 #Correct:2 #Trained:7 Training Accuracy:28.5\r",
      "Progress:8.75 #Correct:2 #Trained:8 Training Accuracy:25.0\r",
      "Progress:10.0 #Correct:3 #Trained:9 Training Accuracy:33.3\r",
      "Progress:11.2 #Correct:3 #Trained:10 Training Accuracy:30.0\r",
      "Progress:12.5 #Correct:3 #Trained:11 Training Accuracy:27.2\r",
      "Progress:13.7 #Correct:4 #Trained:12 Training Accuracy:33.3\r",
      "Progress:15.0 #Correct:4 #Trained:13 Training Accuracy:30.7\r",
      "Progress:16.2 #Correct:5 #Trained:14 Training Accuracy:35.7\r",
      "Progress:17.5 #Correct:6 #Trained:15 Training Accuracy:40.0\r",
      "Progress:18.7 #Correct:7 #Trained:16 Training Accuracy:43.7\r",
      "Progress:20.0 #Correct:8 #Trained:17 Training Accuracy:47.0\r",
      "Progress:21.2 #Correct:9 #Trained:18 Training Accuracy:50.0\r",
      "Progress:22.5 #Correct:9 #Trained:19 Training Accuracy:47.3\r",
      "Progress:23.7 #Correct:10 #Trained:20 Training Accuracy:50.0\r",
      "Progress:25.0 #Correct:11 #Trained:21 Training Accuracy:52.3\r",
      "Progress:26.2 #Correct:12 #Trained:22 Training Accuracy:54.5\r",
      "Progress:27.5 #Correct:13 #Trained:23 Training Accuracy:56.5\r",
      "Progress:28.7 #Correct:13 #Trained:24 Training Accuracy:54.1\r",
      "Progress:30.0 #Correct:14 #Trained:25 Training Accuracy:56.0\r",
      "Progress:31.2 #Correct:15 #Trained:26 Training Accuracy:57.6\n",
      "\r",
      "Progress:32.5 #Correct:15 #Trained:27 Training Accuracy:55.5\r",
      "Progress:33.7 #Correct:16 #Trained:28 Training Accuracy:57.1\r",
      "Progress:35.0 #Correct:17 #Trained:29 Training Accuracy:58.6\r",
      "Progress:36.2 #Correct:18 #Trained:30 Training Accuracy:60.0\r",
      "Progress:37.5 #Correct:19 #Trained:31 Training Accuracy:61.2\r",
      "Progress:38.7 #Correct:20 #Trained:32 Training Accuracy:62.5\r",
      "Progress:40.0 #Correct:21 #Trained:33 Training Accuracy:63.6\r",
      "Progress:41.2 #Correct:22 #Trained:34 Training Accuracy:64.7\r",
      "Progress:42.5 #Correct:23 #Trained:35 Training Accuracy:65.7\r",
      "Progress:43.7 #Correct:23 #Trained:36 Training Accuracy:63.8\r",
      "Progress:45.0 #Correct:24 #Trained:37 Training Accuracy:64.8\r",
      "Progress:46.2 #Correct:25 #Trained:38 Training Accuracy:65.7\r",
      "Progress:47.5 #Correct:26 #Trained:39 Training Accuracy:66.6\r",
      "Progress:48.7 #Correct:27 #Trained:40 Training Accuracy:67.5\r",
      "Progress:50.0 #Correct:28 #Trained:41 Training Accuracy:68.2\r",
      "Progress:51.2 #Correct:29 #Trained:42 Training Accuracy:69.0\r",
      "Progress:52.5 #Correct:30 #Trained:43 Training Accuracy:69.7\r",
      "Progress:53.7 #Correct:31 #Trained:44 Training Accuracy:70.4\r",
      "Progress:55.0 #Correct:32 #Trained:45 Training Accuracy:71.1\r",
      "Progress:56.2 #Correct:33 #Trained:46 Training Accuracy:71.7\r",
      "Progress:57.5 #Correct:34 #Trained:47 Training Accuracy:72.3\r",
      "Progress:58.7 #Correct:35 #Trained:48 Training Accuracy:72.9\r",
      "Progress:60.0 #Correct:36 #Trained:49 Training Accuracy:73.4\r",
      "Progress:61.2 #Correct:37 #Trained:50 Training Accuracy:74.0\r",
      "Progress:62.5 #Correct:38 #Trained:51 Training Accuracy:74.5\n",
      "\r",
      "Progress:63.7 #Correct:39 #Trained:52 Training Accuracy:75.0\r",
      "Progress:65.0 #Correct:40 #Trained:53 Training Accuracy:75.4\r",
      "Progress:66.2 #Correct:41 #Trained:54 Training Accuracy:75.9\r",
      "Progress:67.5 #Correct:42 #Trained:55 Training Accuracy:76.3\r",
      "Progress:68.7 #Correct:43 #Trained:56 Training Accuracy:76.7\r",
      "Progress:70.0 #Correct:44 #Trained:57 Training Accuracy:77.1\r",
      "Progress:71.2 #Correct:45 #Trained:58 Training Accuracy:77.5\r",
      "Progress:72.5 #Correct:46 #Trained:59 Training Accuracy:77.9\r",
      "Progress:73.7 #Correct:47 #Trained:60 Training Accuracy:78.3\r",
      "Progress:75.0 #Correct:48 #Trained:61 Training Accuracy:78.6\r",
      "Progress:76.2 #Correct:49 #Trained:62 Training Accuracy:79.0\r",
      "Progress:77.5 #Correct:50 #Trained:63 Training Accuracy:79.3\r",
      "Progress:78.7 #Correct:51 #Trained:64 Training Accuracy:79.6\r",
      "Progress:80.0 #Correct:52 #Trained:65 Training Accuracy:80.0\r",
      "Progress:81.2 #Correct:53 #Trained:66 Training Accuracy:80.3\r",
      "Progress:82.5 #Correct:54 #Trained:67 Training Accuracy:80.5\r",
      "Progress:83.7 #Correct:55 #Trained:68 Training Accuracy:80.8\r",
      "Progress:85.0 #Correct:56 #Trained:69 Training Accuracy:81.1\r",
      "Progress:86.2 #Correct:57 #Trained:70 Training Accuracy:81.4\r",
      "Progress:87.5 #Correct:58 #Trained:71 Training Accuracy:81.6\r",
      "Progress:88.7 #Correct:59 #Trained:72 Training Accuracy:81.9\r",
      "Progress:90.0 #Correct:60 #Trained:73 Training Accuracy:82.1\r",
      "Progress:91.2 #Correct:61 #Trained:74 Training Accuracy:82.4\r",
      "Progress:92.5 #Correct:62 #Trained:75 Training Accuracy:82.6\r",
      "Progress:93.7 #Correct:63 #Trained:76 Training Accuracy:82.8\n",
      "\r",
      "Progress:95.0 #Correct:64 #Trained:77 Training Accuracy:83.1\r",
      "Progress:96.2 #Correct:65 #Trained:78 Training Accuracy:83.3\r",
      "Progress:97.5 #Correct:66 #Trained:79 Training Accuracy:83.5\r",
      "Progress:98.7 #Correct:67 #Trained:80 Training Accuracy:83.7"
     ]
    }
   ],
   "source": [
    "mlp.train(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress:0.0 #Correct:1 #Tested:1 Testing Accuracy:100.%\r",
      "Progress:5.0 #Correct:2 #Tested:2 Testing Accuracy:100.%\r",
      "Progress:10.0 #Correct:3 #Tested:3 Testing Accuracy:100.%\r",
      "Progress:15.0 #Correct:4 #Tested:4 Testing Accuracy:100.%\r",
      "Progress:20.0 #Correct:5 #Tested:5 Testing Accuracy:100.%\r",
      "Progress:25.0 #Correct:6 #Tested:6 Testing Accuracy:100.%\r",
      "Progress:30.0 #Correct:7 #Tested:7 Testing Accuracy:100.%\r",
      "Progress:35.0 #Correct:8 #Tested:8 Testing Accuracy:100.%\r",
      "Progress:40.0 #Correct:9 #Tested:9 Testing Accuracy:100.%\r",
      "Progress:45.0 #Correct:10 #Tested:10 Testing Accuracy:100.%\r",
      "Progress:50.0 #Correct:11 #Tested:11 Testing Accuracy:100.%\r",
      "Progress:55.0 #Correct:12 #Tested:12 Testing Accuracy:100.%\r",
      "Progress:60.0 #Correct:13 #Tested:13 Testing Accuracy:100.%\r",
      "Progress:65.0 #Correct:14 #Tested:14 Testing Accuracy:100.%\r",
      "Progress:70.0 #Correct:15 #Tested:15 Testing Accuracy:100.%\r",
      "Progress:75.0 #Correct:16 #Tested:16 Testing Accuracy:100.%\r",
      "Progress:80.0 #Correct:17 #Tested:17 Testing Accuracy:100.%\r",
      "Progress:85.0 #Correct:18 #Tested:18 Testing Accuracy:100.%\r",
      "Progress:90.0 #Correct:19 #Tested:19 Testing Accuracy:100.%\r",
      "Progress:95.0 #Correct:20 #Tested:20 Testing Accuracy:100.%"
     ]
    }
   ],
   "source": [
    "# Test after training\n",
    "mlp.test(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
